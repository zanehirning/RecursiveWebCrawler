# RecursiveWebCrawler
This is a web crawler that uses recursive functions to crawl the web. The depth is selected by the user, three is the default (depth is how many links it will travel). The crawler will stop once it has gone full circle and no more links can be visited. (for usage, please only use links you know or that are crawler approved.)
All code can be found in the "master" branch.
